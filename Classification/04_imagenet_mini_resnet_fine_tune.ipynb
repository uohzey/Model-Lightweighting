{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3ad9a0-1871-4f56-88ec-6f35e54a29ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57eede361a4ac9ae633f6dfc5ae51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/34745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940345236584415bb4d0ffbddce47fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/3923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"D:/Study/imagenet_mini/imagenet-mini/\")\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08f7172-72f9-4d9f-b92c-f0163d231d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3db8f41e5643e785566978bd2f62fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniforge3\\envs\\torch\\lib\\site-packages\\PIL\\TiffImagePlugin.py:858: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58d895284d54d5fbaa72836e008e126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3923 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\", cache_dir = \"D:/Study/microsoft/\")\n",
    "size = processor.size[\"shortest_edge\"]\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "\n",
    "train_transforms = Compose([\n",
    "    RandomResizedCrop(size),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(size),\n",
    "    ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# 应用预处理函数\n",
    "def preprocess_train(examples):\n",
    "    examples['pixel_values'] = [train_transforms(img.convert('RGB')) for img in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def preprocess_val(examples):\n",
    "    examples['pixel_values'] = [val_transforms(img.convert('RGB')) for img in examples['image']]\n",
    "    return examples\n",
    "\n",
    "train_dataset = train_ds.map(preprocess_train, batched=True)\n",
    "val_dataset = val_ds.map(preprocess_val, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"pixel_values\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"pixel_values\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f356b2-691f-42c9-a537-1e08ceb38cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ResNetForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93a87ff4-da46-471a-bdc8-895b73209cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da59113510b14bb09df9260741961d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniforge3\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Study\\microsoft\\models--microsoft--resnet-18. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5031eeee8ef3491889e26accb33f3cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = ResNetForImageClassification.from_pretrained(\n",
    "    \"microsoft/resnet-18\",\n",
    "    cache_dir = \"D:/Study/microsoft/\",\n",
    "    num_labels=1000,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# 冻结所有层（默认情况下参数是requires_grad=True）\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# # 仅解冻分类头（最后一层）\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 或者解冻最后几个阶段（例如stage4）\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"stages.3\" in name:  # 根据ResNet结构选择解冻的层\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baffd98e-9888-4baf-b784-ffe1fefedf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: resnet | Type: <class 'transformers.models.resnet.modeling_resnet.ResNetModel'>\n",
      "Layer: classifier | Type: <class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children():\n",
    "    print(f\"Layer: {name} | Type: {type(layer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4da0a2f1-cd13-4b58-ac4d-a7115fb741a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置批量大小（根据GPU显存调整）\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,  # 加速数据加载\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "234a98cf-ac1a-410a-951a-d72a314f50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 交叉熵损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 筛选需要训练的参数\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# 使用AdamW优化器，学习率根据任务调整\n",
    "optimizer = AdamW(trainable_params, lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d55f59d-8865-419c-84c2-ca9ab938cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 1.3100, Val Loss: 1.2795, Val Acc: 0.6882, Epoch Time: 854.44s, Total Time: 854.44s\n",
      "Saved best model!\n",
      "Epoch 2/10, Train Loss: 1.2582, Val Loss: 1.2889, Val Acc: 0.6857, Epoch Time: 857.82s, Total Time: 1712.26s\n",
      "Epoch 3/10, Train Loss: 1.2153, Val Loss: 1.2902, Val Acc: 0.6865, Epoch Time: 846.43s, Total Time: 2558.69s\n",
      "Epoch 4/10, Train Loss: 1.1736, Val Loss: 1.2955, Val Acc: 0.6852, Epoch Time: 856.01s, Total Time: 3414.70s\n",
      "Epoch 5/10, Train Loss: 1.1391, Val Loss: 1.3064, Val Acc: 0.6816, Epoch Time: 846.54s, Total Time: 4261.25s\n",
      "Epoch 6/10, Train Loss: 1.1016, Val Loss: 1.3100, Val Acc: 0.6814, Epoch Time: 848.04s, Total Time: 5109.29s\n",
      "Epoch 7/10, Train Loss: 1.0680, Val Loss: 1.3218, Val Acc: 0.6778, Epoch Time: 847.56s, Total Time: 5956.85s\n",
      "Epoch 8/10, Train Loss: 1.0301, Val Loss: 1.3261, Val Acc: 0.6768, Epoch Time: 858.42s, Total Time: 6815.27s\n",
      "Epoch 9/10, Train Loss: 0.9996, Val Loss: 1.3394, Val Acc: 0.6775, Epoch Time: 879.45s, Total Time: 7694.72s\n",
      "Epoch 10/10, Train Loss: 0.9754, Val Loss: 1.3455, Val Acc: 0.6742, Epoch Time: 859.33s, Total Time: 8554.05s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10  # 根据需求调整\n",
    "best_val_acc = 0.0\n",
    "total_time = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    total_time += epoch_time\n",
    "    \n",
    "    # 计算指标\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    print(\n",
    "        f'Epoch {epoch+1}/{epochs}, '\n",
    "        f'Train Loss: {avg_train_loss:.4f}, '\n",
    "        f'Val Loss: {avg_val_loss:.4f}, '\n",
    "        f'Val Acc: {val_acc:.4f}, '\n",
    "        f'Epoch Time: {epoch_time:.2f}s, '\n",
    "        f'Total Time: {total_time:.2f}s'\n",
    "    )\n",
    "    # 保存最佳模型\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Saved best model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74c1bf-97c9-4f8c-8847-f990fbbb06ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
